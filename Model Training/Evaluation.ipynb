{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook evaluates the localization & diffusion predictions from our trained UNet_locD.  \n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- [1. Import & Model Setup](#Import-&-Model-Setup)  \n",
    "- [2. Data Setup](#Data-Setup)  \n",
    "- [3. Localize](#Localize)  \n",
    "- [4. KNN Matching](#KNN-Matching)  \n",
    "- [5. Evaluation](#Evaluation)  \n",
    "- [6. Visualizations](#Visualizations)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import-&-Model-Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "from model import unet_locD\n",
    "from utils.data_loader import data_loader\n",
    "from utils.loss_calculator import calculate_loss\n",
    "\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data-Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path\n",
    "\n",
    "dir_img = 'path/imgpadding/'        # Set where the image files are, by default should be in the folder imgpadding\n",
    "dir_label = 'path/labelpading/'     # Set where the image files are, by default should be in the folder labelpading\n",
    "dir_pair = 'path/pairpading/'       # Set where the image files are, by default should be in the folder pairpading\n",
    "\n",
    "\n",
    "# Set up the parameter:\n",
    "D_range = [0.01, 2]  # For 1216data\n",
    "\n",
    "label_suffix = '_loc'\n",
    "pair_suffix = '_pair'\n",
    "\n",
    "# load the model \n",
    "dir_model = 'path/model_checkpoint_3.pth'  # Set where the trained model is.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model \n",
    "\n",
    "# 1. load the model \n",
    "# Initialize the model \n",
    "model = unet_locD(n_channels=3, n_classes=1,bilinear=False)\n",
    "# Load the saved state_dict\n",
    "model.load_state_dict(torch.load(dir_model,map_location=torch.device('cpu') ))\n",
    "# Set to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# 3.  Perform evaluation\n",
    "# Set up the device: \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = data_loader(dir_img, dir_label,dir_pair, label_suffix, pair_suffix)\n",
    "dataset = DataLoader(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take one example frame.\n",
    "i = 0\n",
    "example_frame = 3\n",
    "for batch in dataset:\n",
    "    img = batch['image']\n",
    "    labell = batch['label']\n",
    "    pair = batch['pair']\n",
    "    i += 1\n",
    "    if i == example_frame:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The label size is {labell.shape}.')\n",
    "print(f'The image size is {img.shape}')\n",
    "print(f'The particle label pair size is {pair.shape}')\n",
    "\n",
    "# The label size should be 14*64*64\n",
    "# The image size should be 3*64*64\n",
    "# The particle label pair size should be 3 * 10 (# of particles)\n",
    "\n",
    "# If any shape is not right, do permute accordingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labell = labell.permute((0,1,3,2))\n",
    "pair = pair.permute((0,2,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get an output from the model: \n",
    "result = model(img)\n",
    "\n",
    "# Check how it performs: \n",
    "\n",
    "label_data = labell.detach().numpy()[0,:,:,:]\n",
    "label_map = np.sum(label_data, 0)\n",
    "img_data = img.detach().numpy()[0,0,:,:]\n",
    "img_data_sum = np.sum(img.detach().numpy()[0],0)\n",
    "\n",
    "output = torch.sigmoid(result).detach().numpy()[0,:,:,:]\n",
    "output_map = np.sum(output,0)\n",
    "\n",
    "\n",
    "# Pick which image to check \n",
    "plt.imshow(output_map,alpha = 0.5 )  # output results \n",
    "plt.imshow(label_map, alpha = 0.5)   # label \n",
    "plt.imshow(img_data_sum,alpha = 0.5) # image input \n",
    "\n",
    "\n",
    "pair_data = pair.detach().numpy()[0,:,:]\n",
    "for emitter in range(pair_data.shape[1]):\n",
    "    x, y, D= pair_data[:,emitter]\n",
    "    plt.scatter(x - 1, y - 1, color = 'red', s = 5)\n",
    "\n",
    "# If the label and input doesn't match, go back to adjust the permute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same for the D labels.\n",
    "label_mapX = np.sum(label_data,1)\n",
    "\n",
    "outputX = np.sum(output,1)\n",
    "#plt.imshow(outputX,alpha = 0.5 )\n",
    "plt.imshow(label_mapX, alpha = 0.5)    \n",
    "#plt.imshow(img_data,alpha = 1)\n",
    "\n",
    "\n",
    "pair_data = pair.detach().numpy()[0,:,:]\n",
    "for emitter in range(pair_data.shape[1]):\n",
    "    x, y, D= pair_data[:,emitter]\n",
    "    D_cal = (D  - D_range[0])/(D_range[1] - D_range[0]) * 9 + 2\n",
    "    plt.scatter( x-1, D_cal,color = 'red', s = 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Localize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.ndimage import maximum_filter, label, center_of_mass\n",
    "\n",
    "def find_particle_locations(gaussian_map, neighborhood_size = 3, threshold=0.1):\n",
    "    \n",
    "    \"\"\"\n",
    "    Recover particle locations from a Gaussian probability map.\n",
    "    \n",
    "    Args:\n",
    "    - gaussian_map: 3D numpy array of the Gaussian probability map.\n",
    "    - neighborhood_size: Size of the neighborhood to consider for local maxima.\n",
    "    - threshold: Minimum probability to consider a peak as valid.\n",
    "\n",
    "    Returns:\n",
    "    - refined_coords: List of tuples with (x, y) subpixel particle locations.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Find local max \n",
    "    neighborhood = np.ones((neighborhood_size, neighborhood_size,neighborhood_size))\n",
    "    local_max = (gaussian_map == maximum_filter(gaussian_map, footprint=neighborhood))\n",
    "    \n",
    "    # Step 2: Apply threshold to filter noise\n",
    "    threshold_mask = gaussian_map > threshold\n",
    "    valid_peaks = local_max & threshold_mask\n",
    "    \n",
    "    # Step 3: Label connected regions (for subpixel refinement)\n",
    "    labeled_array, num_features = label(valid_peaks)\n",
    "    \n",
    "    # Step 4: Subpixel refinement using center of mass\n",
    "    refined_coords = []\n",
    "    for i in range(1, num_features + 1):  # Label indices start at 1\n",
    "        mask = (labeled_array == i)\n",
    "        if mask.any():\n",
    "            y, x, z = center_of_mass(gaussian_map, mask)  # Compute center of mass\n",
    "            refined_coords.append([x, y, z])\n",
    "    \n",
    "    return refined_coords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Parameters\n",
    "neighborhood_size = 1  # Adjust for PSF size\n",
    "threshold = 0.1  # Adjust based on noise level\n",
    "\n",
    "# Find particle locations\n",
    "particle_locations = find_particle_locations(output.transpose((1,2,0)), neighborhood_size, threshold)\n",
    "\n",
    "\n",
    "plt.imshow(label_map,alpha = 0.5, cmap = 'hot')\n",
    "plt.imshow(img_data, alpha = 0.5, cmap = 'gray')\n",
    "\n",
    "\n",
    "# Print results\n",
    "for i, (x,y,d) in enumerate(particle_locations):\n",
    "    print(f\"Particle {i+1}: x = {x:.2f}, y = {y:.2f}, d = {d:.2f}\")\n",
    "    plt.scatter(x,y , s = 5,color = 'black')\n",
    "\n",
    "\n",
    "# Plot the original label: \n",
    "\n",
    "pair_data = pair.detach().numpy()[0,:,:]\n",
    "for emitter in range(pair_data.shape[1]):\n",
    "    x, y, D = pair_data[:,emitter]\n",
    "    plt.scatter(x-1,y-1, s = 5, color = 'red')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "\n",
    "# Plot particle locations\n",
    "for i, (x, y, d) in enumerate(particle_locations):\n",
    "    plt.scatter(x, y, s=200, edgecolor='red', facecolor='none', linewidth=1)\n",
    "    D_value = (d / 9 ) * (D_range[1] - D_range[0]) + D_range[0]\n",
    "    plt.text(x-2, y+3 , f\"{D_value:.2f}\", color = 'red')\n",
    "\n",
    "img_show = img.detach().numpy()[0,:,:,:]\n",
    "img_show = np.sum(img_show,0)\n",
    "# Display the image\n",
    "plt.imshow(img_show, cmap='gray')\n",
    "\n",
    "# Remove axis labels and ticks\n",
    "plt.axis('off')\n",
    "\n",
    "# Add a scale bar\n",
    "scale_bar_length_pixels = 50  # Length of the scale bar in pixels (e.g., 100 pixels)\n",
    "scale_bar_length_um = scale_bar_length_pixels * 0.1  # Convert pixels to um\n",
    "scale_bar = patches.Rectangle((10, 240), scale_bar_length_pixels, 5, linewidth=0,\n",
    "                               edgecolor=None, facecolor='white')  # Position at (10, 10)\n",
    "plt.gca().add_patch(scale_bar)\n",
    "\n",
    "# Add scale bar text\n",
    "#plt.text(10 + scale_bar_length_pixels / 2, 235, f\"{scale_bar_length_um} μm\", color='white',\n",
    "        # ha='center', fontsize=10)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Parameters\n",
    "neighborhood_size = 1 # Adjust for PSF size\n",
    "threshold = 0.1  # Adjust based on noise level\n",
    "\n",
    "# Find particle locations\n",
    "# particle_locations = find_particle_locations(label_data.transpose((1,2,0)), neighborhood_size, threshold)\n",
    "particle_locations = find_particle_locations(output.transpose((1,2,0)), neighborhood_size, threshold)\n",
    "\n",
    "\n",
    "plt.imshow(label_mapX,alpha = 0.5, cmap = 'hot')\n",
    "# plt.imshow(img_data, alpha = 0.5, cmap = 'gray')\n",
    "\n",
    "\n",
    "# Print results\n",
    "for i, (x, y, d) in enumerate(particle_locations):\n",
    "    print(f\"Particle {i+1}: x = {x:.2f}, y = {y:.2f}\")\n",
    "    plt.scatter(x,d, s = 5,color = 'black')\n",
    "\n",
    "\n",
    "# Plot the original label: \n",
    "\n",
    "\n",
    "pair_data = pair.detach().numpy()[0,:,:]\n",
    "for emitter in range(pair_data.shape[1]):\n",
    "    x, y, D = pair_data[:,emitter]\n",
    "    D = (D - D_range[0])/(D_range[1] - D_range[0]) * 9 + 2\n",
    "    plt.scatter(x-1, D, s = 5, color = 'red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN-Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_location(predicted_pair, true_pair, D_range, threshold=1):\n",
    "    # A->B KNN\n",
    "    nn_model_A_to_B = NearestNeighbors(n_neighbors=1)\n",
    "    nn_model_A_to_B.fit(true_pair[:, :2])\n",
    "    distances_A_to_B, indices_A_to_B = nn_model_A_to_B.kneighbors(predicted_pair[:, :2])\n",
    "\n",
    "    # B->A KNN\n",
    "    nn_model_B_to_A = NearestNeighbors(n_neighbors=1)\n",
    "    nn_model_B_to_A.fit(predicted_pair[:, :2])\n",
    "    distances_B_to_A, indices_B_to_A = nn_model_B_to_A.kneighbors(true_pair[:, :2])\n",
    "\n",
    "    # Flatten since they are (N,1) arrays\n",
    "    distances_A_to_B = distances_A_to_B.ravel()\n",
    "    indices_A_to_B = indices_A_to_B.ravel()\n",
    "    distances_B_to_A = distances_B_to_A.ravel()\n",
    "\n",
    "    # True positives, false positives, and false negatives\n",
    "    true_positive = np.sum(distances_A_to_B < threshold)\n",
    "    false_positive = np.sum(distances_A_to_B >= threshold)\n",
    "    false_negative = np.sum(distances_B_to_A >= threshold)\n",
    "\n",
    "    # Mask for matched predictions (distances under threshold)\n",
    "    mask = distances_A_to_B < threshold\n",
    "\n",
    "    # Extract matched predictions and corresponding true pairs\n",
    "    matched_pred = predicted_pair[mask]\n",
    "    matched_true = true_pair[indices_A_to_B[mask]]\n",
    "\n",
    "    # Compute delta x, delta y\n",
    "    dx = matched_pred[:, 0] - matched_true[:, 0]\n",
    "    dy = matched_pred[:, 1] - matched_true[:, 1]\n",
    "\n",
    "    # Compute predicted diffusion and its difference from truth\n",
    "    #D_pred = (matched_pred[:, 2] / 9.0) * (D_range[1] - D_range[0]) + D_range[0]\n",
    "    D_pred = ((matched_pred[:, 2] - 2) / 9.0) * (D_range[1] - D_range[0]) + D_range[0]\n",
    "    dd = D_pred - matched_true[:, 2]\n",
    "\n",
    "    # Distances for matched points\n",
    "    d = distances_A_to_B[mask]\n",
    "\n",
    "    return (true_positive,\n",
    "            false_positive,\n",
    "            false_negative,\n",
    "            dx.tolist(),\n",
    "            dy.tolist(),\n",
    "            dd.tolist(),\n",
    "            d.tolist(),\n",
    "            D_pred.tolist(),\n",
    "            matched_true[:, 2].tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_positive_sum = 0\n",
    "false_positve_sum = 0\n",
    "false_negative_sum = 0 \n",
    "dx_list_all = []\n",
    "dy_list_all = []\n",
    "dd_list_all = []\n",
    "d_list_all = []\n",
    "\n",
    "D_pred_all = []\n",
    "D_true_all = []\n",
    "# Parameters\n",
    "neighborhood_size = 1  # Adjust for PSF size\n",
    "threshold = 0.1  # Adjust based on noise level  # 0.1 for diffusion prediction\n",
    "\n",
    "dataset =  data_loader(dir_img, dir_label,dir_pair, label_suffix, pair_suffix)\n",
    "val_percent = 0.1\n",
    "n_val = int(len(dataset) * val_percent)\n",
    "n_train = len(dataset) - n_val\n",
    "train_set, val_set = random_split(dataset, [n_train, n_val], generator=torch.Generator().manual_seed(0))\n",
    "val_loader = DataLoader(val_set, shuffle=False, drop_last=True)\n",
    "\n",
    "for batch in val_loader:\n",
    "    img = batch['image']\n",
    "    labell = batch['label']\n",
    "    pair = batch['pair']\n",
    "\n",
    "    # Use the permute as before\n",
    "\n",
    "    labell = labell.permute((0,1,3,2))\n",
    "    pair = pair.permute((0,2,1))\n",
    "\n",
    "\n",
    "    result = model(img)\n",
    "\n",
    "    label_data = labell.detach().numpy()[0]\n",
    "    output = torch.sigmoid(result).detach().numpy()[0]\n",
    "    pair_data = pair.detach().numpy()[0]\n",
    "\n",
    "    \n",
    "\n",
    "    # Find particle locations\n",
    "    \n",
    "    predicted_pair = np.array(find_particle_locations(output.transpose((1,2,0)), neighborhood_size, threshold))\n",
    "    true_pair = []\n",
    "    for emitter in range(pair_data.shape[1]):\n",
    "        x, y, d = pair_data[:,emitter]\n",
    "        #true_pair.append([y-1, x-1, d])   # There's a stupid off set for matlab and python index . Also x and y is reversed. \n",
    "        true_pair.append([x-1, y-1, d])   # xy off set + d off set for one! \n",
    "    true_pair = np.array(true_pair)\n",
    "    if len(predicted_pair) == 0:\n",
    "        predicted_pair = np.zeros([10,3])\n",
    "        \n",
    "    true_positive, false_positve, false_negative, dx_list, dy_list, dd_list, d_list, diff_pred, diff_true = evaluate_location(predicted_pair, true_pair, D_range, threshold= 1.5)\n",
    "\n",
    "    true_positive_sum += true_positive\n",
    "    false_positve_sum += false_positve\n",
    "    false_negative_sum += false_negative\n",
    "    dx_list_all.extend(dx_list)\n",
    "    dy_list_all.extend(dy_list)\n",
    "    d_list_all.extend(d_list)\n",
    "\n",
    "    dd_list_all.extend(dd_list) \n",
    "    D_pred_all.extend(diff_pred)\n",
    "    D_true_all.extend(diff_true)\n",
    "    #break\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score = (2 * true_positive_sum) / (2 * true_positive_sum + false_positve_sum + false_negative_sum)\n",
    "print(f'The f1 score is {f1_score:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dx_array = np.array(dx_list_all) * 100   # nm \n",
    "\n",
    "# Define bin width and calculate bins\n",
    "bin_width = 20\n",
    "bins = np.arange(min(dx_array), max(dx_array) + bin_width, bin_width)\n",
    "\n",
    "# Compute the histogram without density normalization\n",
    "hist, bin_edges = np.histogram(dx_array, bins=bins, density=False)\n",
    "\n",
    "# Normalize the histogram to have a maximum value of 1\n",
    "hist_normalized = hist / hist.max()\n",
    "\n",
    "# Plot the histogram\n",
    "plt.bar(bin_edges[:-1], hist_normalized, width=bin_width, edgecolor='black', alpha=0.7)\n",
    "#plt.title('Normalized Histogram of dx_list_all')\n",
    "plt.xlabel('dx (nm)',fontsize=18)\n",
    "plt.ylabel('Normalized Frequency',fontsize=18)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x localization precision \n",
    "dx_array = np.array(dx_list_all)\n",
    "rmse_x = np.sqrt(np.mean(dx_array**2))\n",
    "print(f\"The x localization error is {rmse_x * 100: .0f} nm.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dy_array = np.array(dy_list_all) * 100  #nm\n",
    "\n",
    "# Define bin width and calculate bins\n",
    "bin_width = 20\n",
    "bins = np.arange(min(dy_array), max(dy_array) + bin_width, bin_width)\n",
    "\n",
    "# Compute the histogram without density normalization\n",
    "hist, bin_edges = np.histogram(dy_array, bins=bins, density=False)\n",
    "\n",
    "# Normalize the histogram to have a maximum value of 1\n",
    "hist_normalized = hist / hist.max()\n",
    "\n",
    "# Plot the histogram\n",
    "plt.bar(bin_edges[:-1], hist_normalized, width=bin_width, edgecolor='black', facecolor = 'orange', alpha=0.7)\n",
    "#plt.title('Normalized Histogram of dx_list_all')\n",
    "plt.xlabel('dy (nm)',fontsize=18)\n",
    "plt.ylabel('Normalized Frequency',fontsize=18)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dy_array = np.array(dy_list_all)\n",
    "rmse_y = np.sqrt(np.mean(dy_array**2))\n",
    "print(f\"The y localization error is {rmse_y * 100: .0f} nm.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the r2:\n",
    "def scatter_with_gaussian_kde( pair, s, diffusion_range):\n",
    "\n",
    "    array = np.array(pair)\n",
    "    xy = np.vstack(array).T\n",
    "    z = gaussian_kde(xy)(xy)\n",
    "    R2_sklearn = r2_score(xy[0], xy[1])\n",
    "\n",
    "    plt.scatter( *zip(*array),c = z, s = s)\n",
    "    plt.ylim([0,2])\n",
    "    plt.xlabel('Ground Truth diffusion coeficient ($μm^2$/s)')\n",
    "    plt.ylabel('Predicted Diffusion coeficient($μm^2$/s) ')\n",
    "    return R2_sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diffusion pair: \n",
    "diffusion_pair = [(a,b) for a,b in zip( D_true_all,D_pred_all)]\n",
    "\n",
    "R2 = scatter_with_gaussian_kde(diffusion_pair,10,D_range)\n",
    "\n",
    "plt.plot([0,2],[0,2], color = 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The R^2 value is {R2:.2f}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sim_psf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
